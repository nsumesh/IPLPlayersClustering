{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9277058,"sourceType":"datasetVersion","datasetId":5614827},{"sourceId":9277061,"sourceType":"datasetVersion","datasetId":5614830}],"dockerImageVersionId":30761,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current sessionP","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-29T22:34:26.778062Z","iopub.execute_input":"2024-08-29T22:34:26.778363Z","iopub.status.idle":"2024-08-29T22:34:27.139095Z","shell.execute_reply.started":"2024-08-29T22:34:26.778317Z","shell.execute_reply":"2024-08-29T22:34:27.138118Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/bowlers-data-2016to2022/final_aggregated_bowler_data.csv\n/kaggle/input/batsman-data-2016to2022/final_aggregated_batsman_data.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Obtaining data from csv below which has been cleaned up\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nipl_batsman_stats_data = pd.read_csv('/kaggle/input/batsman-data-2016to2022/final_aggregated_batsman_data.csv')\nipl_bowler_stats_data =  pd.read_csv('/kaggle/input/bowlers-data-2016to2022/final_aggregated_bowler_data.csv')\nadvanced_features_batting = ['Player','Mat','Inns','Avg', 'SR', 'BF','4s','6s'] \nadvanced_features_bowling = ['Player','Mat','Inns','Avg', 'Econ', 'SR'] \ndata_for_clustering_batsman = ipl_batsman_stats_data[advanced_features_batting]\ndata_for_clustering_bowler = ipl_bowler_stats_data[advanced_features_bowling]\nprint(data_for_clustering_batsman.head())\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-29T22:34:27.140877Z","iopub.execute_input":"2024-08-29T22:34:27.141394Z","iopub.status.idle":"2024-08-29T22:34:27.190895Z","shell.execute_reply.started":"2024-08-29T22:34:27.141348Z","shell.execute_reply":"2024-08-29T22:34:27.190019Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"           Player  Mat  Inns        Avg          SR    BF   4s   6s\n0  Shikhar Dhawan  110   110  38.561429  129.822857  2825  404   82\n1     Virat Kohli  100   100  41.030000  130.228571  2618  302  108\n2    David Warner   79    79  49.925000  138.330000  2368  327  113\n3        KL Rahul   69    67  54.270000  141.684000  2094  258  126\n4    Rishabh Pant   98    97  34.004286  146.625714  1918  260  129\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Here, we normalize the data using min-max scaling and then accordingly rank the players based on a Weighted Perfomance Score \n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Load the data\nipl_batsman_stats_data = pd.read_csv('/kaggle/input/batsman-data-2016to2022/final_aggregated_batsman_data.csv')\nipl_bowler_stats_data = pd.read_csv('/kaggle/input/bowlers-data-2016to2022/final_aggregated_bowler_data.csv')\n\nadvanced_features_batting = ['Player', 'Mat', 'Inns', 'Avg', 'SR', 'BF', '4s', '6s', 'Runs']\nadvanced_features_bowling = ['Player', 'Mat', 'Inns', 'Avg', 'Econ', 'SR', 'Wkts']\n\ndata_for_clustering_batsman = ipl_batsman_stats_data[advanced_features_batting].copy()\ndata_for_clustering_bowler = ipl_bowler_stats_data[advanced_features_bowling].copy()\n\nscaler = MinMaxScaler()\n\ndata_for_clustering_batsman.loc[:, advanced_features_batting[2:]] = scaler.fit_transform(\n    data_for_clustering_batsman.loc[:, advanced_features_batting[2:]]\n)\n\ndata_for_clustering_bowler.loc[:, advanced_features_bowling[2:]] = scaler.fit_transform(\n    data_for_clustering_bowler.loc[:, advanced_features_bowling[2:]]\n)\n\n# Define a weighted performance metric for batsmen\ndata_for_clustering_batsman['Weighted_Performance_Score'] = (\n    data_for_clustering_batsman['Mat'] * (\n        data_for_clustering_batsman['Avg'] + \n        data_for_clustering_batsman['SR'] + \n        data_for_clustering_batsman['BF'] + \n        data_for_clustering_batsman['4s'] + \n        data_for_clustering_batsman['6s'] +\n        data_for_clustering_batsman['Runs']\n    )\n)\n\ndata_for_clustering_bowler['Weighted_Performance_Score'] = (\n    data_for_clustering_bowler['Mat'] * (\n        data_for_clustering_bowler['Avg'] + \n        data_for_clustering_bowler['Econ'] + \n        data_for_clustering_bowler['SR'] +\n        data_for_clustering_bowler['Wkts']\n    )\n)\n\nsorted_batsmen = data_for_clustering_batsman.sort_values(by='Weighted_Performance_Score', ascending=False).reset_index(drop=True)\nsorted_bowlers = data_for_clustering_bowler.sort_values(by='Weighted_Performance_Score', ascending=False).reset_index(drop=True)\n\n# Assign tiers (10 players per tier)\ndef assign_tiers(df, tier_size=10):\n    df['Tier'] = (df.index // tier_size) + 1\n    return df\n\nsorted_batsmen = assign_tiers(sorted_batsmen)\nsorted_bowlers = assign_tiers(sorted_bowlers)\n\n# Reset the index to start from 1\nsorted_batsmen.index = sorted_batsmen.index + 1\nsorted_bowlers.index = sorted_bowlers.index + 1\n\n# Display the top tiers\nprint(\"Top 3 Tiers for Batsmen:\")\nprint(sorted_batsmen.head(30))\n\nprint(\"\\nTop 3 Tiers for Bowlers:\")\nprint(sorted_bowlers.head(30))\n\nsorted_batsmen.to_csv('/kaggle/working/batsmen_tiers.csv', index=True)\nsorted_bowlers.to_csv('/kaggle/working/bowlers_tiers.csv', index=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-29T22:34:27.192124Z","iopub.execute_input":"2024-08-29T22:34:27.192418Z","iopub.status.idle":"2024-08-29T22:34:28.343377Z","shell.execute_reply.started":"2024-08-29T22:34:27.192385Z","shell.execute_reply":"2024-08-29T22:34:28.342470Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Top 3 Tiers for Batsmen:\n              Player  Mat      Inns       Avg        SR        BF        4s  \\\n1     Shikhar Dhawan  110  1.000000  0.710548  0.324557  1.000000  1.000000   \n2        Virat Kohli  100  0.908257  0.756035  0.325571  0.926700  0.747525   \n3       Rishabh Pant   98  0.880734  0.626576  0.366564  0.678824  0.643564   \n4       Sanju Samson  100  0.908257  0.572587  0.350943  0.702195  0.542079   \n5       David Warner   79  0.715596  0.919937  0.345825  0.838173  0.809406   \n6       Rohit Sharma   99  0.889908  0.518782  0.318471  0.691218  0.606436   \n7        Jos Buttler   82  0.733945  0.712469  0.373479  0.669263  0.685644   \n8           MS Dhoni  105  0.816514  0.790703  0.315257  0.544263  0.316832   \n9     Dinesh Karthik  107  0.926606  0.617705  0.357118  0.537535  0.514851   \n10    AB de Villiers   80  0.697248  0.780265  0.390383  0.576487  0.482673   \n11  Suryakumar Yadav   93  0.761468  0.575456  0.337457  0.601275  0.633663   \n12          KL Rahul   69  0.605505  1.000000  0.354210  0.741147  0.638614   \n13       Nitish Rana   91  0.770642  0.539840  0.338061  0.575071  0.470297   \n14     Ambati Rayudu   92  0.770642  0.548580  0.314429  0.614731  0.443069   \n15     Hardik Pandya   98  0.834862  0.550238  0.348025  0.448654  0.339109   \n16      Shreyas Iyer   87  0.788991  0.546132  0.293404  0.662535  0.485149   \n17    Kieron Pollard   96  0.779817  0.547448  0.364407  0.402266  0.242574   \n18     Robin Uthappa   85  0.743119  0.461581  0.335675  0.514164  0.457921   \n19     Manish Pandey   83  0.697248  0.645792  0.311304  0.583569  0.410891   \n20     Andre Russell   76  0.587156  0.579172  0.427467  0.327904  0.245050   \n21   Kane Williamson   74  0.660550  0.651110  0.305707  0.580382  0.448020   \n22      Ishan Kishan   75  0.633028  0.505646  0.316818  0.500000  0.410891   \n23     Krunal Pandya   98  0.770642  0.438101  0.341079  0.343130  0.299505   \n24   Ravindra Jadeja   99  0.697248  0.671879  0.329843  0.297805  0.207921   \n25      Shubman Gill   74  0.642202  0.601290  0.319790  0.536827  0.465347   \n26    Rahul Tripathi   76  0.669725  0.508046  0.344512  0.451841  0.435644   \n27    Ajinkya Rahane   77  0.660550  0.451236  0.289464  0.546034  0.495050   \n28      Suresh Raina   73  0.651376  0.543431  0.325630  0.494334  0.457921   \n29     Glenn Maxwell   78  0.669725  0.462164  0.363933  0.376062  0.334158   \n30   Quinton de Kock   64  0.577982  0.600995  0.324785  0.531161  0.475248   \n\n          6s      Runs  Weighted_Performance_Score  Tier  \n1   0.535948  1.000000                  502.815791     1  \n2   0.705882  0.951952                  441.366484     1  \n3   0.843137  0.774775                  385.477233     1  \n4   0.836601  0.758122                  376.252740     1  \n5   0.738562  0.916189                  360.879276     1  \n6   0.607843  0.680863                  338.937665     1  \n7   0.882353  0.772864                  335.877880     1  \n8   0.673203  0.543544                  334.299106     1  \n9   0.503268  0.592138                  334.119853     1  \n10  0.967320  0.707617                  312.379693     1  \n11  0.464052  0.634180                  301.885744     2  \n12  0.823529  0.803986                  300.942561     2  \n13  0.725490  0.595414                  295.219728     2  \n14  0.653595  0.600328                  292.075250     2  \n15  0.666667  0.505324                  280.085637     2  \n16  0.509804  0.638002                  272.747160     2  \n17  0.745098  0.453453                  264.503724     2  \n18  0.562092  0.533989                  243.560795     2  \n19  0.405229  0.567022                  242.675947     2  \n20  1.000000  0.450177                  230.262470     2  \n21  0.411765  0.565111                  219.194970     3  \n22  0.555556  0.510511                  209.956604     3  \n23  0.326797  0.361998                  206.839788     3  \n24  0.267974  0.305487                  206.009896     3  \n25  0.307190  0.518701                  203.436629     3  \n26  0.444444  0.490854                  203.326034     3  \n27  0.248366  0.512968                  195.820024     3  \n28  0.346405  0.499317                  194.693832     3  \n29  0.516340  0.431340                  193.751911     3  \n30  0.490196  0.539176                  189.539881     3  \n\nTop 3 Tiers for Bowlers:\n                 Player  Mat      Inns       Avg      Econ        SR  \\\n1        Jasprit Bumrah  103  1.000000  0.142194  0.277668  0.187813   \n2      Yuzvendra Chahal  101  0.980392  0.144137  0.309155  0.180282   \n3       Ravindra Jadeja   99  0.921569  0.297463  0.323382  0.347831   \n4          Sunil Narine   93  0.892157  0.252137  0.271020  0.334762   \n5            Axar Patel   91  0.872549  0.280297  0.296560  0.349683   \n6         Krunal Pandya   98  0.901961  0.266320  0.291662  0.335097   \n7     Bhuvneshwar Kumar   87  0.843137  0.232023  0.302974  0.286931   \n8           Rashid Khan   92  0.892157  0.134933  0.214694  0.205041   \n9        Mohammad Shami   78  0.754902  0.240297  0.419009  0.240106   \n10  Ravichandran Ashwin   87  0.843137  0.259760  0.308571  0.321214   \n11       Sandeep Sharma   75  0.725490  0.283154  0.333994  0.332540   \n12       Shardul Thakur   74  0.705882  0.201067  0.427483  0.202942   \n13          Trent Boult   71  0.686275  0.220503  0.393819  0.233951   \n14        Andre Russell   76  0.647059  0.148520  0.460272  0.141584   \n15         Dwayne Bravo   70  0.676471  0.187440  0.399592  0.196337   \n16       Marcus Stoinis   67  0.480392  0.301440  0.507638  0.277672   \n17        Kagiso Rabada   63  0.607843  0.147776  0.372408  0.162123   \n18        Hardik Pandya   72  0.598039  0.229216  0.391510  0.246000   \n19       Mohammed Siraj   65  0.627451  0.239507  0.418231  0.245823   \n20        Kuldeep Yadav   59  0.549020  0.305360  0.367483  0.346296   \n21        Deepak Chahar   61  0.588235  0.270256  0.367837  0.287975   \n22       Jaydev Unadkat   56  0.539216  0.272067  0.433605  0.269136   \n23          Umesh Yadav   60  0.578431  0.185536  0.382367  0.197605   \n24        Harshal Patel   48  0.460784  0.315714  0.453528  0.295326   \n25      Prasidh Krishna   51  0.490196  0.319200  0.434776  0.318025   \n26         Deepak Hooda   65  0.186275  0.264528  0.472816  0.264198   \n27        Glenn Maxwell   67  0.392157  0.244928  0.316898  0.292543   \n28         Chris Morris   54  0.519608  0.180040  0.374422  0.194856   \n29        Piyush Chawla   54  0.509804  0.226387  0.409796  0.234506   \n30     Moises Henriques   34  0.323529  0.751547  0.314558  0.724280   \n\n        Wkts  Weighted_Performance_Score  Tier  \n1   1.000000                  165.590522     1  \n2   0.977444                  162.712763     1  \n3   0.466165                  142.049247     1  \n4   0.578947                  133.628615     1  \n5   0.526316                  132.209827     1  \n6   0.451128                  131.732250     1  \n7   0.684211                  131.034037     1  \n8   0.834586                  127.811444     1  \n9   0.676692                  122.936068     1  \n10  0.496241                  120.563384     1  \n11  0.556391                  112.955934     2  \n12  0.601504                  106.041690     2  \n13  0.616541                  104.001799     2  \n14  0.541353                   98.171469     2  \n15  0.578947                   95.362166     2  \n16  0.248120                   89.436340     2  \n17  0.736842                   89.406433     2  \n18  0.360902                   88.389249     2  \n19  0.436090                   87.077330     2  \n20  0.451128                   86.745759     2  \n21  0.436090                   83.091654     3  \n22  0.436090                   79.010296     3  \n23  0.526316                   77.509444     3  \n24  0.496241                   74.918825     3  \n25  0.360902                   73.078025     3  \n26  0.060150                   69.009995     3  \n27  0.172932                   68.829200     3  \n28  0.496241                   67.260150     3  \n29  0.353383                   66.099900     3  \n30  0.120301                   64.963293     3  \n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/2305980265.py:18: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[1.         0.90825688 0.71559633 0.60550459 0.88073394 0.73394495\n 0.90825688 0.69724771 0.88990826 0.78899083 0.76146789 0.7706422\n 0.7706422  0.9266055  0.69724771 0.66055046 0.81651376 0.57798165\n 0.74311927 0.64220183 0.66055046 0.63302752 0.48623853 0.83486239\n 0.65137615 0.66972477 0.5412844  0.77981651 0.58715596 0.56880734\n 0.66972477 0.58715596 0.47706422 0.58715596 0.55963303 0.7706422\n 0.34862385 0.34862385 0.41284404 0.32110092 0.4587156  0.48623853\n 0.69724771 0.41284404 0.59633028 0.32110092 0.53211009 0.65137615\n 0.3853211  0.37614679 0.39449541 0.37614679 0.6146789  0.39449541\n 0.40366972 0.29357798 0.41284404 0.35779817 0.3853211  0.40366972\n 0.29357798 0.30275229 0.2293578  0.27522936 0.12844037 0.34862385\n 0.13761468 0.21100917 0.17431193 0.19266055 0.20183486 0.20183486\n 0.33944954 0.1559633  0.33027523 0.12844037 0.22018349 0.23853211\n 0.26605505 0.13761468 0.39449541 0.14678899 0.11926606 0.22018349\n 0.24770642 0.09174312 0.11926606 0.3853211  0.26605505 0.14678899\n 0.20183486 0.11009174 0.29357798 0.3853211  0.18348624 0.11926606\n 0.16513761 0.11009174 0.1559633  0.12844037 0.08256881 0.20183486\n 0.05504587 0.14678899 0.12844037 0.10091743 0.08256881 0.20183486\n 0.13761468 0.11009174 0.10091743 0.17431193 0.16513761 0.19266055\n 0.0733945  0.08256881 0.20183486 0.05504587 0.0733945  0.16513761\n 0.2293578  0.10091743 0.19266055 0.11926606 0.11926606 0.10091743\n 0.17431193 0.20183486 0.09174312 0.05504587 0.19266055 0.11926606\n 0.08256881 0.08256881 0.04587156 0.03669725 0.06422018 0.17431193\n 0.08256881 0.09174312 0.29357798 0.0733945  0.08256881 0.04587156\n 0.16513761 0.05504587 0.05504587 0.16513761 0.0733945  0.0733945\n 0.1559633  0.13761468 0.2293578  0.14678899 0.17431193 0.1559633\n 0.11009174 0.08256881 0.0733945  0.03669725 0.05504587 0.10091743\n 0.10091743 0.03669725 0.11009174 0.09174312 0.02752294 0.18348624\n 0.03669725 0.03669725 0.0733945  0.04587156 0.05504587 0.06422018\n 0.1559633  0.01834862 0.1559633  0.04587156 0.1559633  0.08256881\n 0.03669725 0.10091743 0.04587156 0.04587156 0.03669725 0.06422018\n 0.01834862 0.02752294 0.11009174 0.03669725 0.06422018 0.01834862\n 0.01834862 0.17431193 0.06422018 0.00917431 0.03669725 0.0733945\n 0.08256881 0.0733945  0.13761468 0.11009174 0.00917431 0.00917431\n 0.00917431 0.05504587 0.00917431 0.01834862 0.06422018 0.06422018\n 0.02752294 0.00917431 0.01834862 0.04587156 0.02752294 0.01834862\n 0.00917431 0.06422018 0.03669725 0.01834862 0.         0.01834862\n 0.01834862 0.04587156 0.01834862 0.08256881 0.00917431 0.03669725\n 0.04587156 0.01834862 0.0733945  0.02752294 0.05504587 0.01834862\n 0.01834862 0.00917431 0.02752294 0.03669725 0.03669725 0.00917431\n 0.01834862 0.01834862 0.         0.04587156 0.00917431 0.03669725\n 0.04587156 0.00917431 0.00917431 0.00917431 0.0733945  0.04587156\n 0.         0.00917431 0.         0.02752294 0.00917431 0.00917431\n 0.         0.02752294 0.01834862 0.01834862 0.03669725 0.02752294\n 0.05504587 0.00917431 0.05504587 0.         0.06422018 0.00917431\n 0.01834862 0.00917431 0.0733945  0.03669725 0.0733945  0.\n 0.03669725 0.08256881 0.01834862 0.00917431 0.00917431 0.\n 0.         0.03669725 0.04587156 0.02752294 0.         0.01834862\n 0.         0.00917431 0.00917431 0.00917431 0.         0.00917431\n 0.01834862 0.         0.00917431 0.         0.         0.\n 0.02752294 0.00917431 0.         0.         0.03669725 0.\n 0.         0.         0.         0.00917431 0.02752294 0.01834862\n 0.01834862 0.02752294 0.         0.         0.00917431 0.00917431\n 0.         0.01834862 0.00917431 0.00917431 0.00917431 0.\n 0.         0.         0.01834862 0.00917431 0.         0.00917431\n 0.         0.         0.         0.00917431 0.         0.\n 0.         0.        ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  data_for_clustering_batsman.loc[:, advanced_features_batting[2:]] = scaler.fit_transform(\n/tmp/ipykernel_37/2305980265.py:18: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[1.00000000e+00 9.26699717e-01 8.38172805e-01 7.41147309e-01\n 6.78824363e-01 6.69263456e-01 7.02195467e-01 5.76487252e-01\n 6.91218130e-01 6.62535411e-01 6.01274788e-01 6.14730878e-01\n 5.75070822e-01 5.37535411e-01 5.83569405e-01 5.80382436e-01\n 5.44263456e-01 5.31161473e-01 5.14164306e-01 5.36827195e-01\n 5.46033994e-01 5.00000000e-01 4.93980170e-01 4.48654391e-01\n 4.94334278e-01 4.51841360e-01 4.41572238e-01 4.02266289e-01\n 3.27903683e-01 3.81019830e-01 3.76062323e-01 3.98016997e-01\n 4.28116147e-01 3.99787535e-01 3.91288952e-01 3.43130312e-01\n 3.20113314e-01 3.13739377e-01 3.58356941e-01 3.27549575e-01\n 3.11614731e-01 3.14447592e-01 2.97804533e-01 2.90722380e-01\n 2.97450425e-01 3.14093484e-01 2.75495751e-01 2.08215297e-01\n 2.76558074e-01 2.41855524e-01 2.13172805e-01 2.23441926e-01\n 2.28753541e-01 1.92988669e-01 2.14943343e-01 2.00424929e-01\n 2.22379603e-01 2.14943343e-01 2.11756374e-01 1.95467422e-01\n 1.82365439e-01 1.74220963e-01 1.68555241e-01 1.78116147e-01\n 1.60764873e-01 1.86260623e-01 1.43767705e-01 1.33144476e-01\n 1.68909348e-01 1.61473088e-01 1.16147309e-01 1.43413598e-01\n 1.18626062e-01 1.38810198e-01 1.47662890e-01 1.20396601e-01\n 1.36685552e-01 1.37039660e-01 1.38810198e-01 1.29603399e-01\n 1.22875354e-01 1.27478754e-01 1.15793201e-01 1.11189802e-01\n 1.27832861e-01 9.87960340e-02 1.06940510e-01 1.01274788e-01\n 8.71104816e-02 9.87960340e-02 7.93201133e-02 8.99433428e-02\n 9.31303116e-02 7.25920680e-02 7.93201133e-02 8.32152975e-02\n 8.28611898e-02 6.90509915e-02 7.82577904e-02 7.33002833e-02\n 7.08215297e-02 6.83427762e-02 6.09065156e-02 7.68413598e-02\n 6.79886686e-02 5.87818697e-02 5.02832861e-02 6.62181303e-02\n 4.74504249e-02 7.71954674e-02 6.90509915e-02 5.59490085e-02\n 7.29461756e-02 6.58640227e-02 4.53257790e-02 5.20538244e-02\n 4.35552408e-02 5.48866856e-02 3.11614731e-02 3.85977337e-02\n 6.23229462e-02 5.41784703e-02 5.06373938e-02 3.89518414e-02\n 4.17847025e-02 4.95750708e-02 5.55949008e-02 4.49716714e-02\n 4.56798867e-02 3.96600567e-02 4.39093484e-02 5.09915014e-02\n 4.85127479e-02 3.25779037e-02 4.14305949e-02 4.00141643e-02\n 4.21388102e-02 5.02832861e-02 3.89518414e-02 4.07223796e-02\n 4.35552408e-02 4.17847025e-02 3.75354108e-02 3.50566572e-02\n 3.71813031e-02 3.47025496e-02 2.62039660e-02 3.50566572e-02\n 3.32861190e-02 3.64730878e-02 4.46175637e-02 3.29320113e-02\n 3.36402266e-02 3.68271955e-02 3.82436261e-02 3.18696884e-02\n 2.65580737e-02 3.43484419e-02 2.47875354e-02 3.43484419e-02\n 1.69971671e-02 1.98300283e-02 2.69121813e-02 2.69121813e-02\n 3.22237960e-02 2.26628895e-02 2.12464589e-02 2.23087819e-02\n 1.62889518e-02 2.05382436e-02 1.55807365e-02 1.98300283e-02\n 1.91218130e-02 1.98300283e-02 1.94759207e-02 1.41643059e-02\n 1.91218130e-02 1.73512748e-02 1.66430595e-02 1.66430595e-02\n 1.52266289e-02 1.94759207e-02 1.52266289e-02 1.59348442e-02\n 1.91218130e-02 1.27478754e-02 9.91501416e-03 1.38101983e-02\n 1.52266289e-02 9.56090652e-03 9.56090652e-03 1.66430595e-02\n 1.16855524e-02 1.52266289e-02 1.45184136e-02 1.27478754e-02\n 1.23937677e-02 1.55807365e-02 9.91501416e-03 1.23937677e-02\n 2.93909348e-02 1.73512748e-02 1.52266289e-02 1.34560907e-02\n 9.91501416e-03 1.27478754e-02 9.20679887e-03 9.91501416e-03\n 1.20396601e-02 1.38101983e-02 1.73512748e-02 9.20679887e-03\n 1.02691218e-02 6.37393768e-03 1.06232295e-02 9.20679887e-03\n 6.01983003e-03 9.91501416e-03 1.31019830e-02 1.13314448e-02\n 8.85269122e-03 7.79036827e-03 6.72804533e-03 1.20396601e-02\n 8.14447592e-03 1.27478754e-02 4.60339943e-03 7.08215297e-03\n 9.91501416e-03 5.31161473e-03 1.20396601e-02 9.91501416e-03\n 1.13314448e-02 8.14447592e-03 1.02691218e-02 3.89518414e-03\n 7.08215297e-03 6.72804533e-03 6.72804533e-03 4.24929178e-03\n 4.95750708e-03 8.85269122e-03 8.14447592e-03 4.95750708e-03\n 5.31161473e-03 8.85269122e-03 8.14447592e-03 6.72804533e-03\n 1.02691218e-02 5.31161473e-03 1.06232295e-02 8.49858357e-03\n 2.83286119e-03 8.14447592e-03 4.24929178e-03 3.54107649e-03\n 4.24929178e-03 8.85269122e-03 3.54107649e-03 6.37393768e-03\n 4.24929178e-03 2.47875354e-03 7.79036827e-03 5.31161473e-03\n 5.31161473e-03 5.66572238e-03 4.95750708e-03 3.89518414e-03\n 7.43626062e-03 3.18696884e-03 6.72804533e-03 4.60339943e-03\n 8.49858357e-03 8.85269122e-03 4.60339943e-03 2.83286119e-03\n 3.18696884e-03 8.14447592e-03 3.54107649e-03 1.77053824e-03\n 2.12464589e-03 1.41643059e-03 1.77053824e-03 9.20679887e-03\n 3.89518414e-03 5.31161473e-03 4.24929178e-03 2.47875354e-03\n 1.41643059e-03 2.12464589e-03 5.66572238e-03 7.08215297e-04\n 2.12464589e-03 5.66572238e-03 2.83286119e-03 6.37393768e-03\n 1.77053824e-03 3.89518414e-03 1.41643059e-03 1.06232295e-03\n 4.24929178e-03 2.47875354e-03 7.08215297e-04 3.54107649e-04\n 4.24929178e-03 0.00000000e+00 2.47875354e-03 1.06232295e-03\n 1.41643059e-03 2.47875354e-03 2.47875354e-03 2.12464589e-03\n 1.41643059e-03 3.18696884e-03 1.41643059e-03 1.06232295e-03\n 1.06232295e-03 3.54107649e-04 2.47875354e-03 1.06232295e-03\n 1.06232295e-03 1.06232295e-03 0.00000000e+00 3.54107649e-04\n 1.41643059e-03 0.00000000e+00 1.41643059e-03 7.08215297e-04\n 7.08215297e-04 0.00000000e+00 1.06232295e-03 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 3.54107649e-04\n 0.00000000e+00 1.06232295e-03]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  data_for_clustering_batsman.loc[:, advanced_features_batting[2:]] = scaler.fit_transform(\n/tmp/ipykernel_37/2305980265.py:18: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[1.         0.74752475 0.80940594 0.63861386 0.64356436 0.68564356\n 0.54207921 0.48267327 0.60643564 0.48514851 0.63366337 0.44306931\n 0.47029703 0.51485149 0.41089109 0.4480198  0.31683168 0.47524752\n 0.45792079 0.46534653 0.4950495  0.41089109 0.42821782 0.33910891\n 0.45792079 0.43564356 0.3539604  0.24257426 0.2450495  0.47524752\n 0.33415842 0.37128713 0.34653465 0.34405941 0.37128713 0.29950495\n 0.32920792 0.31188119 0.33415842 0.27970297 0.28217822 0.1980198\n 0.20792079 0.34653465 0.17574257 0.30445545 0.20544554 0.27227723\n 0.25990099 0.19554455 0.12871287 0.18811881 0.11881188 0.13613861\n 0.16089109 0.20544554 0.15594059 0.13366337 0.11386139 0.14108911\n 0.11386139 0.15841584 0.15346535 0.16089109 0.11138614 0.13861386\n 0.14851485 0.16584158 0.14851485 0.13366337 0.09405941 0.15346535\n 0.09405941 0.09158416 0.0990099  0.11633663 0.12376238 0.0990099\n 0.10643564 0.12128713 0.07920792 0.10891089 0.12128713 0.11633663\n 0.08910891 0.07425743 0.07178218 0.07178218 0.05940594 0.08663366\n 0.07425743 0.09653465 0.06435644 0.04950495 0.04455446 0.07425743\n 0.04455446 0.05940594 0.03217822 0.03712871 0.0470297  0.04455446\n 0.05445545 0.03712871 0.05693069 0.02475248 0.05445545 0.02722772\n 0.03712871 0.04455446 0.03960396 0.02970297 0.02227723 0.03217822\n 0.03960396 0.02970297 0.02722772 0.02475248 0.02970297 0.03217822\n 0.03960396 0.04950495 0.03217822 0.02475248 0.03960396 0.0470297\n 0.04455446 0.02722772 0.02722772 0.03465347 0.03217822 0.01980198\n 0.05693069 0.01732673 0.03217822 0.03465347 0.02475248 0.03465347\n 0.02722772 0.02722772 0.03465347 0.02475248 0.02475248 0.03465347\n 0.01485149 0.02722772 0.03465347 0.02475248 0.01485149 0.00742574\n 0.01980198 0.01485149 0.01732673 0.02227723 0.02475248 0.01732673\n 0.01485149 0.01732673 0.01980198 0.02227723 0.00742574 0.0049505\n 0.01732673 0.02227723 0.00990099 0.00742574 0.00990099 0.01237624\n 0.01237624 0.01732673 0.01485149 0.01237624 0.01237624 0.01237624\n 0.01237624 0.0049505  0.00990099 0.00742574 0.0049505  0.01237624\n 0.01485149 0.00990099 0.00247525 0.00990099 0.00990099 0.00247525\n 0.0049505  0.00742574 0.00247525 0.00742574 0.00742574 0.01237624\n 0.00990099 0.00742574 0.00742574 0.00742574 0.0049505  0.00990099\n 0.00742574 0.00990099 0.         0.0049505  0.00247525 0.00742574\n 0.00990099 0.00742574 0.0049505  0.         0.00247525 0.0049505\n 0.0049505  0.00742574 0.00742574 0.0049505  0.00247525 0.00742574\n 0.0049505  0.00742574 0.00990099 0.00247525 0.00247525 0.00990099\n 0.00247525 0.0049505  0.00247525 0.0049505  0.0049505  0.\n 0.         0.0049505  0.00742574 0.0049505  0.00247525 0.0049505\n 0.0049505  0.0049505  0.00742574 0.0049505  0.         0.00247525\n 0.00247525 0.00247525 0.0049505  0.0049505  0.0049505  0.00247525\n 0.00247525 0.00247525 0.00742574 0.         0.00247525 0.0049505\n 0.         0.0049505  0.00247525 0.00247525 0.0049505  0.00247525\n 0.0049505  0.00247525 0.0049505  0.         0.         0.\n 0.00247525 0.         0.0049505  0.00247525 0.         0.00247525\n 0.         0.00247525 0.00247525 0.00247525 0.         0.00247525\n 0.         0.         0.         0.         0.0049505  0.\n 0.0049505  0.         0.         0.00247525 0.         0.\n 0.         0.         0.         0.         0.00247525 0.\n 0.         0.00247525 0.00247525 0.         0.00247525 0.\n 0.         0.00247525 0.00247525 0.00247525 0.         0.00247525\n 0.00247525 0.00247525 0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.        ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  data_for_clustering_batsman.loc[:, advanced_features_batting[2:]] = scaler.fit_transform(\n/tmp/ipykernel_37/2305980265.py:18: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0.53594771 0.70588235 0.73856209 0.82352941 0.84313725 0.88235294\n 0.83660131 0.96732026 0.60784314 0.50980392 0.46405229 0.65359477\n 0.7254902  0.50326797 0.40522876 0.41176471 0.67320261 0.49019608\n 0.5620915  0.30718954 0.24836601 0.55555556 0.4379085  0.66666667\n 0.34640523 0.44444444 0.83006536 0.74509804 1.         0.35947712\n 0.51633987 0.38562092 0.22875817 0.52941176 0.18300654 0.32679739\n 0.35947712 0.41176471 0.23529412 0.28104575 0.32026144 0.28104575\n 0.26797386 0.16993464 0.29411765 0.09150327 0.31372549 0.39869281\n 0.16339869 0.20915033 0.4248366  0.34640523 0.29411765 0.33986928\n 0.20261438 0.26143791 0.1503268  0.23529412 0.18300654 0.22222222\n 0.24836601 0.16339869 0.23529412 0.18954248 0.19607843 0.12418301\n 0.1372549  0.15686275 0.07843137 0.1372549  0.26143791 0.14379085\n 0.19607843 0.1503268  0.14379085 0.1503268  0.13071895 0.13071895\n 0.07189542 0.08496732 0.13071895 0.05882353 0.05882353 0.07189542\n 0.07189542 0.1372549  0.10457516 0.12418301 0.16993464 0.07843137\n 0.11764706 0.05882353 0.05882353 0.14379085 0.11764706 0.03267974\n 0.07189542 0.09803922 0.12418301 0.08496732 0.09150327 0.08496732\n 0.07843137 0.05882353 0.05882353 0.14379085 0.07843137 0.10457516\n 0.11764706 0.04575163 0.02614379 0.09150327 0.05228758 0.06535948\n 0.08496732 0.05228758 0.09150327 0.05228758 0.10457516 0.07843137\n 0.03921569 0.03267974 0.06535948 0.10457516 0.05882353 0.05228758\n 0.0130719  0.05228758 0.04575163 0.07189542 0.03921569 0.06535948\n 0.0130719  0.07189542 0.03921569 0.01960784 0.05882353 0.02614379\n 0.03267974 0.         0.00653595 0.04575163 0.01960784 0.01960784\n 0.03921569 0.03267974 0.01960784 0.02614379 0.03267974 0.05228758\n 0.0130719  0.02614379 0.03921569 0.01960784 0.0130719  0.0130719\n 0.03267974 0.0130719  0.01960784 0.0130719  0.03921569 0.03921569\n 0.0130719  0.00653595 0.0130719  0.01960784 0.01960784 0.03267974\n 0.02614379 0.0130719  0.0130719  0.00653595 0.01960784 0.00653595\n 0.0130719  0.03267974 0.00653595 0.01960784 0.0130719  0.0130719\n 0.00653595 0.0130719  0.03267974 0.01960784 0.0130719  0.01960784\n 0.02614379 0.00653595 0.01960784 0.01960784 0.01960784 0.\n 0.00653595 0.         0.00653595 0.00653595 0.00653595 0.00653595\n 0.         0.         0.         0.00653595 0.         0.\n 0.         0.         0.00653595 0.00653595 0.00653595 0.\n 0.         0.         0.00653595 0.01960784 0.0130719  0.00653595\n 0.00653595 0.         0.         0.0130719  0.00653595 0.\n 0.00653595 0.00653595 0.00653595 0.         0.0130719  0.0130719\n 0.00653595 0.00653595 0.         0.00653595 0.         0.\n 0.         0.00653595 0.         0.         0.00653595 0.00653595\n 0.00653595 0.00653595 0.         0.00653595 0.         0.\n 0.00653595 0.00653595 0.         0.00653595 0.         0.\n 0.0130719  0.         0.00653595 0.00653595 0.         0.\n 0.         0.         0.         0.00653595 0.         0.\n 0.         0.00653595 0.         0.         0.00653595 0.\n 0.         0.         0.         0.         0.         0.\n 0.00653595 0.         0.00653595 0.00653595 0.         0.00653595\n 0.         0.         0.         0.         0.         0.\n 0.00653595 0.00653595 0.         0.00653595 0.         0.\n 0.00653595 0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.        ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  data_for_clustering_batsman.loc[:, advanced_features_batting[2:]] = scaler.fit_transform(\n/tmp/ipykernel_37/2305980265.py:18: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[1.00000000e+00 9.51951952e-01 9.16188916e-01 8.03985804e-01\n 7.74774775e-01 7.72863773e-01 7.58121758e-01 7.07616708e-01\n 6.80862681e-01 6.38001638e-01 6.34179634e-01 6.00327600e-01\n 5.95413595e-01 5.92137592e-01 5.67021567e-01 5.65110565e-01\n 5.43543544e-01 5.39175539e-01 5.33988534e-01 5.18700519e-01\n 5.12967513e-01 5.10510511e-01 5.06142506e-01 5.05323505e-01\n 4.99317499e-01 4.90854491e-01 4.82118482e-01 4.53453453e-01\n 4.50177450e-01 4.33524434e-01 4.31340431e-01 4.27518428e-01\n 4.16052416e-01 4.10046410e-01 3.75102375e-01 3.61998362e-01\n 3.52443352e-01 3.45345345e-01 3.43980344e-01 3.29511330e-01\n 3.22140322e-01 3.10128310e-01 3.05487305e-01 2.99754300e-01\n 2.96205296e-01 2.95932296e-01 2.92110292e-01 2.68905269e-01\n 2.66448266e-01 2.51160251e-01 2.48976249e-01 2.48430248e-01\n 2.36691237e-01 2.26863227e-01 2.19492219e-01 2.18400218e-01\n 2.09664210e-01 2.01747202e-01 1.99563200e-01 1.97106197e-01\n 1.87824188e-01 1.82091182e-01 1.78542179e-01 1.77723178e-01\n 1.68168168e-01 1.67076167e-01 1.57521158e-01 1.53699154e-01\n 1.53153153e-01 1.50696151e-01 1.50150150e-01 1.49331149e-01\n 1.44144144e-01 1.43871144e-01 1.42506143e-01 1.38684139e-01\n 1.38138138e-01 1.37319137e-01 1.33770134e-01 1.27764128e-01\n 1.24761125e-01 1.20120120e-01 1.15479115e-01 1.14933115e-01\n 1.11930112e-01 1.10292110e-01 1.08381108e-01 1.08381108e-01\n 1.03467103e-01 1.01283101e-01 9.20010920e-02 8.98170898e-02\n 8.68140868e-02 8.54490854e-02 8.27190827e-02 7.69860770e-02\n 7.61670762e-02 7.37100737e-02 7.37100737e-02 7.28910729e-02\n 7.04340704e-02 6.96150696e-02 6.87960688e-02 6.85230685e-02\n 6.85230685e-02 6.82500683e-02 6.38820639e-02 6.30630631e-02\n 6.27900628e-02 6.25170625e-02 6.19710620e-02 6.16980617e-02\n 5.89680590e-02 5.81490581e-02 5.62380562e-02 5.59650560e-02\n 5.32350532e-02 5.21430521e-02 5.10510511e-02 5.07780508e-02\n 5.07780508e-02 5.02320502e-02 4.99590500e-02 4.94130494e-02\n 4.91400491e-02 4.83210483e-02 4.58640459e-02 4.53180453e-02\n 4.39530440e-02 4.39530440e-02 4.39530440e-02 4.34070434e-02\n 4.28610429e-02 4.09500410e-02 4.04040404e-02 3.95850396e-02\n 3.95850396e-02 3.87660388e-02 3.84930385e-02 3.57630358e-02\n 3.54900355e-02 3.52170352e-02 3.46710347e-02 3.46710347e-02\n 3.24870325e-02 3.13950314e-02 2.94840295e-02 2.94840295e-02\n 2.89380289e-02 2.89380289e-02 2.86650287e-02 2.81190281e-02\n 2.78460278e-02 2.67540268e-02 2.62080262e-02 2.53890254e-02\n 2.48430248e-02 2.45700246e-02 2.32050232e-02 2.29320229e-02\n 2.23860224e-02 2.15670216e-02 2.12940213e-02 2.12940213e-02\n 2.04750205e-02 2.04750205e-02 1.99290199e-02 1.96560197e-02\n 1.88370188e-02 1.82910183e-02 1.82910183e-02 1.80180180e-02\n 1.77450177e-02 1.66530167e-02 1.66530167e-02 1.58340158e-02\n 1.50150150e-02 1.47420147e-02 1.44690145e-02 1.44690145e-02\n 1.41960142e-02 1.39230139e-02 1.39230139e-02 1.39230139e-02\n 1.36500137e-02 1.36500137e-02 1.28310128e-02 1.22850123e-02\n 1.20120120e-02 1.20120120e-02 1.17390117e-02 1.17390117e-02\n 1.14660115e-02 1.11930112e-02 1.11930112e-02 1.09200109e-02\n 1.09200109e-02 1.06470106e-02 1.03740104e-02 9.82800983e-03\n 9.55500956e-03 9.55500956e-03 9.00900901e-03 9.00900901e-03\n 9.00900901e-03 9.00900901e-03 8.73600874e-03 8.73600874e-03\n 8.73600874e-03 8.73600874e-03 8.46300846e-03 8.46300846e-03\n 8.46300846e-03 8.46300846e-03 7.91700792e-03 7.91700792e-03\n 7.64400764e-03 7.37100737e-03 7.09800710e-03 7.09800710e-03\n 7.09800710e-03 7.09800710e-03 6.82500683e-03 6.55200655e-03\n 6.55200655e-03 6.55200655e-03 6.55200655e-03 6.55200655e-03\n 6.27900628e-03 6.27900628e-03 6.27900628e-03 6.00600601e-03\n 6.00600601e-03 6.00600601e-03 5.73300573e-03 5.73300573e-03\n 5.46000546e-03 5.46000546e-03 5.18700519e-03 5.18700519e-03\n 5.18700519e-03 5.18700519e-03 5.18700519e-03 4.91400491e-03\n 4.91400491e-03 4.91400491e-03 4.91400491e-03 4.91400491e-03\n 4.91400491e-03 4.91400491e-03 4.36800437e-03 4.36800437e-03\n 4.36800437e-03 4.09500410e-03 4.09500410e-03 4.09500410e-03\n 4.09500410e-03 4.09500410e-03 3.82200382e-03 3.82200382e-03\n 3.82200382e-03 3.54900355e-03 3.54900355e-03 3.54900355e-03\n 3.54900355e-03 3.54900355e-03 3.27600328e-03 3.27600328e-03\n 3.27600328e-03 3.27600328e-03 3.27600328e-03 3.00300300e-03\n 3.00300300e-03 2.73000273e-03 2.73000273e-03 2.73000273e-03\n 2.45700246e-03 2.45700246e-03 2.18400218e-03 2.18400218e-03\n 2.18400218e-03 2.18400218e-03 2.18400218e-03 2.18400218e-03\n 2.18400218e-03 2.18400218e-03 1.91100191e-03 1.91100191e-03\n 1.91100191e-03 1.91100191e-03 1.91100191e-03 1.91100191e-03\n 1.91100191e-03 1.91100191e-03 1.91100191e-03 1.91100191e-03\n 1.63800164e-03 1.63800164e-03 1.63800164e-03 1.36500137e-03\n 1.36500137e-03 1.36500137e-03 1.36500137e-03 1.36500137e-03\n 1.09200109e-03 1.09200109e-03 1.09200109e-03 1.09200109e-03\n 1.09200109e-03 1.09200109e-03 1.09200109e-03 8.19000819e-04\n 8.19000819e-04 8.19000819e-04 8.19000819e-04 5.46000546e-04\n 5.46000546e-04 5.46000546e-04 5.46000546e-04 5.46000546e-04\n 5.46000546e-04 2.73000273e-04 2.73000273e-04 2.73000273e-04\n 2.73000273e-04 2.73000273e-04 2.73000273e-04 2.73000273e-04\n 2.73000273e-04 2.73000273e-04 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n 0.00000000e+00 0.00000000e+00]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  data_for_clustering_batsman.loc[:, advanced_features_batting[2:]] = scaler.fit_transform(\n/tmp/ipykernel_37/2305980265.py:22: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[1.         0.98039216 0.89215686 0.60784314 0.84313725 0.75490196\n 0.68627451 0.70588235 0.89215686 0.67647059 0.7254902  0.64705882\n 0.87254902 0.57843137 0.46078431 0.51960784 0.84313725 0.92156863\n 0.90196078 0.54901961 0.53921569 0.58823529 0.62745098 0.41176471\n 0.51960784 0.53921569 0.42156863 0.41176471 0.59803922 0.49019608\n 0.50980392 0.31372549 0.34313725 0.43137255 0.29411765 0.33333333\n 0.28431373 0.3627451  0.37254902 0.28431373 0.40196078 0.46078431\n 0.35294118 0.33333333 0.35294118 0.33333333 0.40196078 0.37254902\n 0.35294118 0.48039216 0.47058824 0.29411765 0.2254902  0.33333333\n 0.3627451  0.2254902  0.19607843 0.30392157 0.39215686 0.34313725\n 0.2254902  0.14705882 0.24509804 0.35294118 0.12745098 0.39215686\n 0.15686275 0.35294118 0.18627451 0.25490196 0.20588235 0.21568627\n 0.12745098 0.26470588 0.21568627 0.24509804 0.26470588 0.15686275\n 0.26470588 0.18627451 0.32352941 0.18627451 0.12745098 0.15686275\n 0.17647059 0.17647059 0.11764706 0.20588235 0.19607843 0.11764706\n 0.14705882 0.07843137 0.07843137 0.12745098 0.10784314 0.20588235\n 0.11764706 0.14705882 0.24509804 0.11764706 0.07843137 0.10784314\n 0.05882353 0.07843137 0.12745098 0.08823529 0.08823529 0.08823529\n 0.15686275 0.10784314 0.07843137 0.08823529 0.04901961 0.06862745\n 0.14705882 0.10784314 0.05882353 0.05882353 0.09803922 0.06862745\n 0.10784314 0.17647059 0.18627451 0.06862745 0.11764706 0.06862745\n 0.08823529 0.03921569 0.05882353 0.1372549  0.05882353 0.08823529\n 0.11764706 0.06862745 0.09803922 0.07843137 0.04901961 0.04901961\n 0.07843137 0.06862745 0.12745098 0.04901961 0.11764706 0.12745098\n 0.14705882 0.04901961 0.03921569 0.05882353 0.09803922 0.04901961\n 0.01960784 0.18627451 0.04901961 0.10784314 0.03921569 0.03921569\n 0.02941176 0.02941176 0.05882353 0.03921569 0.03921569 0.07843137\n 0.03921569 0.15686275 0.03921569 0.01960784 0.05882353 0.05882353\n 0.03921569 0.04901961 0.01960784 0.03921569 0.03921569 0.01960784\n 0.02941176 0.03921569 0.02941176 0.03921569 0.05882353 0.07843137\n 0.02941176 0.01960784 0.01960784 0.00980392 0.03921569 0.02941176\n 0.00980392 0.01960784 0.00980392 0.01960784 0.04901961 0.00980392\n 0.03921569 0.03921569 0.04901961 0.00980392 0.02941176 0.00980392\n 0.01960784 0.00980392 0.00980392 0.03921569 0.00980392 0.00980392\n 0.01960784 0.02941176 0.00980392 0.02941176 0.04901961 0.\n 0.00980392 0.00980392 0.         0.02941176 0.00980392 0.\n 0.00980392 0.00980392 0.02941176 0.00980392 0.01960784 0.\n 0.04901961 0.00980392 0.         0.01960784 0.01960784]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  data_for_clustering_bowler.loc[:, advanced_features_bowling[2:]] = scaler.fit_transform(\n/tmp/ipykernel_37/2305980265.py:22: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[1.         0.97744361 0.83458647 0.73684211 0.68421053 0.67669173\n 0.61654135 0.60150376 0.57894737 0.57894737 0.55639098 0.54135338\n 0.52631579 0.52631579 0.4962406  0.4962406  0.4962406  0.46616541\n 0.45112782 0.45112782 0.43609023 0.43609023 0.43609023 0.42857143\n 0.42105263 0.40601504 0.39097744 0.38345865 0.36090226 0.36090226\n 0.35338346 0.35338346 0.34586466 0.33834586 0.33834586 0.33834586\n 0.31578947 0.31578947 0.30827068 0.30827068 0.30827068 0.29323308\n 0.29323308 0.27819549 0.27067669 0.26315789 0.2556391  0.2556391\n 0.2556391  0.2481203  0.2406015  0.23308271 0.23308271 0.22556391\n 0.22556391 0.21804511 0.21804511 0.21804511 0.21052632 0.20300752\n 0.19548872 0.18796992 0.18796992 0.18045113 0.18045113 0.17293233\n 0.17293233 0.17293233 0.16541353 0.16541353 0.15789474 0.15037594\n 0.15037594 0.14285714 0.14285714 0.13533835 0.13533835 0.13533835\n 0.12781955 0.12781955 0.12030075 0.12030075 0.12030075 0.12030075\n 0.12030075 0.11278195 0.11278195 0.10526316 0.10526316 0.09774436\n 0.09774436 0.09774436 0.09022556 0.09022556 0.09022556 0.09022556\n 0.09022556 0.09022556 0.09022556 0.08270677 0.08270677 0.08270677\n 0.07518797 0.07518797 0.07518797 0.07518797 0.07518797 0.06766917\n 0.06766917 0.06766917 0.06015038 0.06015038 0.06015038 0.06015038\n 0.06015038 0.06015038 0.06015038 0.06015038 0.06015038 0.06015038\n 0.06015038 0.06015038 0.06015038 0.05263158 0.05263158 0.05263158\n 0.05263158 0.05263158 0.05263158 0.05263158 0.05263158 0.05263158\n 0.05263158 0.04511278 0.04511278 0.04511278 0.04511278 0.04511278\n 0.04511278 0.04511278 0.04511278 0.04511278 0.03759398 0.03759398\n 0.03759398 0.03759398 0.03759398 0.03759398 0.03759398 0.03759398\n 0.03759398 0.03759398 0.03759398 0.03759398 0.03007519 0.03007519\n 0.03007519 0.03007519 0.03007519 0.03007519 0.03007519 0.03007519\n 0.02255639 0.02255639 0.02255639 0.02255639 0.02255639 0.02255639\n 0.02255639 0.02255639 0.01503759 0.01503759 0.01503759 0.01503759\n 0.01503759 0.01503759 0.01503759 0.01503759 0.01503759 0.01503759\n 0.01503759 0.0075188  0.0075188  0.0075188  0.0075188  0.0075188\n 0.0075188  0.0075188  0.0075188  0.0075188  0.0075188  0.0075188\n 0.0075188  0.0075188  0.0075188  0.0075188  0.0075188  0.0075188\n 0.0075188  0.0075188  0.0075188  0.0075188  0.0075188  0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.         0.\n 0.         0.         0.         0.         0.        ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  data_for_clustering_bowler.loc[:, advanced_features_bowling[2:]] = scaler.fit_transform(\n","output_type":"stream"}]}]}